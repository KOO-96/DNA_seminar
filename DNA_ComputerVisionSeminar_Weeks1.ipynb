{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOslNBmpIAm99ZK5ZrJw7AD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOO-96/DNA_seminar/blob/main/DNA_ComputerVisionSeminar_Weeks1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weeks1\n",
        "1. 머신러닝과 딥러닝.\n",
        "2. 인공신경망\n",
        "3. CNN\n",
        "4. 기울기 소실과 과적합(Overfitting) 문제"
      ],
      "metadata": {
        "id": "QZzvnxb_AMEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 머신러닝과 딥러닝\n",
        "\n",
        "> 정형 데이터 -> 머신러닝이 유리함\n",
        "\n",
        "> 비정형 데이터 -> 딥러닝이 유리함  \n",
        "  -> 트리(Tree) 모델: 비선형데이터에 어느정도는 Good ; 데이터도 상대적으로 많이 필요하진 않음., 설명력이 좋은편.\n",
        "\n",
        "  - 딥러닝: 데이터가 많이 필요하다. 비선형데이터에도 활용하기 좋다. 단점으로는 Overfitting시 train데이터에 대한 의존도가 높아지게 된다."
      ],
      "metadata": {
        "id": "7-sL_DdoCfnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 독립적인 feature를 찾는 것이 중요했음."
      ],
      "metadata": {
        "id": "RFTXzmxQCfq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 인공신경망\n",
        "- Relu함수: 기울기 값이 소실되지 않음.   \n",
        "  -> 대부분 He 초기값을 w(가중치)값으로 지정함."
      ],
      "metadata": {
        "id": "INjcTX1SCfuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. CNN\n",
        "- C축을 늘려가기. Padding을 통해 사이즈 손실 방지, Max Pooling을 너무 크게 하면 좋지 않음. ( 2X2를 사용하는 것이 좋음)"
      ],
      "metadata": {
        "id": "QutMkH1LI4EZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Global Average Pooling\n",
        "  1. 파라미터 수를 많이 줄일 수 있음.\n",
        "  2. 공간정보 손실을 막을 수 있음."
      ],
      "metadata": {
        "id": "CSPResjlQM3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- B H W C  -> 파이토치"
      ],
      "metadata": {
        "id": "aR8U4PIVQjk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 기울기 소실 방지\n",
        "- ReLu 활성화 함수 \n",
        "\n",
        "- 가중치 초기화\n",
        "  - w 값에 따라 학습의 가능/불가능을 판가름.\n",
        "  - He초기값. -> w의 초기값을 설정.\n",
        "\n",
        "- Batch Normalization \n",
        "  - 매 layer마다 입력되는 값의 범위를 제한. -> 기울기 소실 방지 \n",
        "\n",
        "- Drop Out\n",
        "  - 인공신경망이 Deep -> 과적합 - 중간 중간 노드를 종료함 = 앙상블 효과. \n",
        "  - 편차를 줄여준다. = 과적합을 방지한다. \n",
        "  - Training 할때만 사용."
      ],
      "metadata": {
        "id": "YuyuRVocFoPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Week1 과제."
      ],
      "metadata": {
        "id": "WVo9Hd4KK8_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/uzairrj/beg-tut-intel-image-classification-93-76-accur/notebook"
      ],
      "metadata": {
        "id": "lDQN5MhCMBTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Models.Sequential()\n",
        "\n",
        "# model.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu',padding = 'same',input_shape=(150,150,3)))\n",
        "# model.add(Layers.Conv2D(256,kernel_size=(3,3),padding = 'same',activation='relu'))\n",
        "# model.add(Layers.MaxPool2D(5,5))\n",
        "\n",
        "# model.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu',padding = 'same'))\n",
        "# model.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu',padding = 'same'))\n",
        "# model.add(Layers.MaxPool2D(5,5))\n",
        "\n",
        "# model.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu',padding = 'same'))\n",
        "# model.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu',padding = 'same'))\n",
        "# model.add(Layers.MaxPool2D(5,5))\n",
        "\n",
        "# model.add(Layers.Flatten())\n",
        "# model.add(Layers.Dense(100,activation='relu'))\n",
        "# model.add(Layers.Dense(50,activation='relu'))\n",
        "# model.add(Layers.Dropout(rate=0.5))\n",
        "# model.add(Layers.Dense(6,activation='softmax'))\n",
        "\n",
        "# model.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
        "# Utils.plot_model(model,to_file='model.png',show_shapes=True)"
      ],
      "metadata": {
        "id": "l7AmxOXtLVMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot.plot(trained.history['acc'])\n",
        "# plot.plot(trained.history['val_acc'])\n",
        "# plot.title('Model accuracy')\n",
        "# plot.ylabel('Accuracy')\n",
        "# plot.xlabel('Epoch')\n",
        "# plot.legend(['Train', 'Test'], loc='upper left')\n",
        "# plot.show()\n",
        "\n",
        "# plot.plot(trained.history['loss'])\n",
        "# plot.plot(trained.history['val_loss'])\n",
        "# plot.title('Model loss')\n",
        "# plot.ylabel('Loss')\n",
        "# plot.xlabel('Epoch')\n",
        "# plot.legend(['Train', 'Test'], loc='upper left')\n",
        "# plot.show()"
      ],
      "metadata": {
        "id": "nTmrt0osLfWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_images,test_labels = get_images('../input/seg_test/seg_test/')\n",
        "# test_images = np.array(test_images)\n",
        "# test_labels = np.array(test_labels)\n",
        "# model.evaluate(test_images,test_labels, verbose=1)"
      ],
      "metadata": {
        "id": "Gk0VWAI_Lfaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 원본 0.8426 -> 0.8687   : 소폭 상승시켰음.\n",
        "-> 코드를 작성할 때 파라미터를 줄이고 층을 늘리는 방향으로 생각하며 접근. 효율적인 모델을 만들려고 했음.\n",
        "-> 추후 다른 부분에 활용할 계획."
      ],
      "metadata": {
        "id": "ULGFDTG8LjwR"
      }
    }
  ]
}